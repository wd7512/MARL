"""
Agent implementations for multi-agent electricity market simulation.
"""

import torch
from abc import ABC, abstractmethod
from stable_baselines3 import PPO
from typing import Callable


class BaseAgent(ABC):
    """Abstract base class for all market agents."""

    def __init__(self, env=None):
        self.env = env
        self.null_action = [0.0, 0.0]

    @abstractmethod
    def act(self, obs, deterministic=True):
        """Return an action given an observation."""
        pass

    @abstractmethod
    def fixed_act_function(self, deterministic=True) -> Callable:
        """Return a frozen action function for use by other agents."""
        pass

    def train(self, *args, **kwargs):
        """Optional: Override if the agent can learn."""
        raise NotImplementedError("This agent type does not support training.")


class SimpleAgent(BaseAgent):
    """
    Simple baseline agent that bids at Short-Run Marginal Cost (SRMC).
    Always offers full capacity at cost price.
    """

    def __init__(self, env=None):
        super().__init__(env)

    def act(self, obs, deterministic=True):
        """Returns null action: [0.0, 0.0] = (full capacity, zero price delta)"""
        return self.null_action

    def fixed_act_function(self, deterministic=True) -> Callable:
        """Return a frozen action function for use by other agents."""
        # Capture current null_action by value
        frozen_action = list(self.null_action)

        def act_fn(obs):
            return frozen_action

        return act_fn


class PPOAgent(BaseAgent):
    """
    Proximal Policy Optimization agent for strategic bidding.
    Uses a neural network policy to learn optimal bidding strategies.
    """

    def __init__(
        self,
        env,
        seed=42,
        learning_rate=3e-4,
        gamma=0.98,
        clip_range=0.2,
        n_steps=2048,
        batch_size=64,
        hidden_sizes=(64, 64),
        weight_decay=0.0,
        **kwargs,
    ):
        """
        Initialize PPO agent with configurable hyperparameters.

        Args:
            env: Gym environment instance
            seed: Random seed for reproducibility
            learning_rate: Learning rate for optimizer
            gamma: Discount factor for future rewards
            clip_range: PPO clipping parameter
            n_steps: Number of steps to collect before update
            batch_size: Minibatch size for updates
            hidden_sizes: Tuple of hidden layer sizes
            weight_decay: L2 regularization coefficient
        """
        super().__init__(env)

        # Neural network architecture
        policy_kwargs = dict(
            activation_fn=torch.nn.ReLU,
            net_arch=dict(pi=list(hidden_sizes), vf=list(hidden_sizes)),
        )

        # Initialize PPO model
        self.model = PPO(
            "MlpPolicy",
            env,
            verbose=0,
            learning_rate=learning_rate,
            gamma=gamma,
            clip_range=clip_range,
            n_steps=n_steps,
            batch_size=batch_size,
            seed=seed,
            policy_kwargs=policy_kwargs,
            **kwargs,
        )

        # Add L2 regularization if specified
        if weight_decay > 0:
            self.model.policy.optimizer = torch.optim.Adam(
                self.model.policy.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay,
            )

    def train(
        self,
        total_timesteps=50000,
        checkpoint_path=None,
        checkpoint_freq=1000,
        callbacks=None,
    ):
        """
        Train the agent using PPO algorithm.

        Args:
            total_timesteps: Total number of environment steps
            checkpoint_path: Directory to save checkpoints
            checkpoint_freq: Frequency of checkpoint saves
            callbacks: List of callback objects
        """
        from stable_baselines3.common.callbacks import CheckpointCallback

        callback_list = callbacks if callbacks is not None else []

        if checkpoint_path is not None:
            checkpoint_cb = CheckpointCallback(
                save_freq=checkpoint_freq,
                save_path=checkpoint_path,
                name_prefix="ppo_agent",
            )
            callback_list.append(checkpoint_cb)

        self.model.learn(total_timesteps=total_timesteps, callback=callback_list)

    def act(self, obs, deterministic=True):
        """
        Return action for given observation.

        Args:
            obs: Environment observation
            deterministic: If True, use deterministic policy

        Returns:
            action: [q_fraction, price_delta_param]
        """
        action, _ = self.model.predict(obs, deterministic=deterministic)
        return action

    def fixed_act_function(self, deterministic=True) -> Callable:
        """
        Return a frozen action function for use by other agents.

        Returns:
            Callable that maps observation to action
        """
        # BUG: This function is not frozen. When the agent is trained, the model changes, and so does the action function.

        # TODO: Consider caching actions for efficiency
        def act_fn(obs):
            action, _ = self.model.predict(obs, deterministic=deterministic)
            return action

        return act_fn

    def save(self, path):
        """Save model to disk."""
        self.model.save(path)

    def load(self, path):
        """Load model from disk."""
        self.model = PPO.load(path, env=self.env)

    def save_to_bytes(self) -> bytes:
        """
        Serialize agent state to bytes.

        Use with load_from_bytes() to mutate an existing instance,
        or with from_bytes() to create a new instance.

        Returns:
            Serialized model state including weights, optimizer, and RNG.
        """
        import io

        buffer = io.BytesIO()
        self.model.save(buffer)
        return buffer.getvalue()

    def load_from_bytes(self, data: bytes):
        """
        Load serialized state into this existing instance (in-place mutation).

        Use this when you have a live agent that you want to restore to a
        previous state, e.g., restoring opponent snapshots in SBR training.

        Args:
            data: Bytes from save_to_bytes().
        """
        import io

        buffer = io.BytesIO(data)
        self.model = PPO.load(buffer, env=self.env)

    @classmethod
    def from_bytes(cls, data: bytes, env) -> "PPOAgent":
        """
        Create a new agent instance directly from serialized state.

        Use this instead of __init__ + load_from_bytes when creating agents
        in worker processes. This avoids the overhead and potential RNG
        perturbation of initializing a random model that gets immediately
        discarded.

        Args:
            data: Bytes from save_to_bytes().
            env: Environment instance for the agent.

        Returns:
            New PPOAgent instance with the deserialized model.
        """
        import io

        instance = cls.__new__(cls)
        instance.env = env
        instance.null_action = [0.0, 0.0]

        buffer = io.BytesIO(data)
        instance.model = PPO.load(buffer, env=env)

        return instance
